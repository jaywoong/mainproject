{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pred_machine.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv0jZezPSORM"
      },
      "source": [
        "!python -m pip install schedule\n",
        "!python -m pip install pystan\n",
        "!python -m pip install fbprophet\n",
        "!python -m pip install finance-datareader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzzOn-ULWESk"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import schedule\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import FinanceDataReader as fdr\n",
        "import matplotlib.pyplot as plt\n",
        "from time import sleep\n",
        "from fbprophet import Prophet\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Lambda\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH7Gs_GvYir-"
      },
      "source": [
        "data = pd.read_excel('./samsung.xlsx')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLX0LYErn7u9"
      },
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(1000)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYhyzKc_xTwP"
      },
      "source": [
        "def pred_machine(data):\n",
        "  scaler = MinMaxScaler()\n",
        "  scale_cols = list(data.columns[1:])\n",
        "  scaled = scaler.fit_transform(data[scale_cols])\n",
        "  df = pd.DataFrame(scaled, columns=scale_cols)\n",
        "  x_train, x_test, y_train, y_test = train_test_split(df.drop('y', 1), df['y'], test_size=0.2, random_state=0, shuffle=False)\n",
        "\n",
        "  WINDOW_SIZE=120\n",
        "  BATCH_SIZE=32\n",
        "\n",
        "  train_data = windowed_dataset(y_train, WINDOW_SIZE, BATCH_SIZE, True)\n",
        "  test_data = windowed_dataset(y_test, WINDOW_SIZE, BATCH_SIZE, False)\n",
        "\n",
        "  model = Sequential([\n",
        "      Conv1D(filters=32, kernel_size=5,\n",
        "            padding=\"causal\",\n",
        "            activation=\"relu\",\n",
        "            input_shape=[WINDOW_SIZE, 1]),\n",
        "      LSTM(16, activation='tanh'),\n",
        "      Dense(16, activation=\"relu\"),\n",
        "      Dense(1),\n",
        "  ])\n",
        "\n",
        "  loss = Huber()\n",
        "  optimizer = Adam(0.0005)\n",
        "  model.compile(loss=Huber(), optimizer=optimizer, metrics=['mse'])\n",
        "\n",
        "  earlystopping = EarlyStopping(monitor='val_loss', patience=100, mode='min')\n",
        "  filename = os.path.join('tmp', 'ckeckpointer.ckpt')\n",
        "  checkpoint = ModelCheckpoint(filename, \n",
        "                              save_weights_only=True, \n",
        "                              save_best_only=True, \n",
        "                              monitor='val_loss', \n",
        "                              verbose=1)\n",
        "\n",
        "  history = model.fit(train_data, \n",
        "                      validation_data=(test_data), \n",
        "                      epochs=500, \n",
        "                      callbacks=[checkpoint, earlystopping])\n",
        "\n",
        "  for i in range(10):\n",
        "    merge_data = pd.DataFrame()\n",
        "    for col in data:\n",
        "      if col != 'DATE' and col != 'y':\n",
        "        data_copy = data[['DATE', col, 'DATE']].copy()\n",
        "        data_copy.columns = ['ds', 'y', 'DATE']\n",
        "        data_copy = data_copy.set_index('DATE')\n",
        "\n",
        "        prophet = Prophet(seasonality_mode='multiplicative', \n",
        "                      yearly_seasonality=True,\n",
        "                      weekly_seasonality=True, daily_seasonality=True,\n",
        "                      changepoint_prior_scale=0.5)\n",
        "        prophet.fit(data_copy)\n",
        "\n",
        "        future_data = prophet.make_future_dataframe(periods=1, freq='d')\n",
        "        forecast_data = prophet.predict(future_data)\n",
        "        forecast_copy = pd.DataFrame(forecast_data[['ds', 'yhat']].tail(1))\n",
        "        forecast_copy.columns = ['DATE', col]\n",
        "\n",
        "        merge_data[col] = forecast_copy[col]\n",
        "    merge_data['DATE'] = forecast_copy['DATE']\n",
        "    df_row = pd.concat([data, merge_data])\n",
        "\n",
        "    pred_scaled = scaler.fit_transform(df_row[scale_cols])\n",
        "    pred_df = pd.DataFrame(pred_scaled, columns=scale_cols)\n",
        "    p_x_train, p_x_test, p_y_train, p_y_test = train_test_split(pred_df.drop('y', 1), pred_df['y'], test_size=0.2, random_state=0, shuffle=False)\n",
        "    WINDOW_SIZE=120\n",
        "    BATCH_SIZE=32\n",
        "    pred_train = windowed_dataset(p_y_train, WINDOW_SIZE, BATCH_SIZE, True)\n",
        "    pred_test = windowed_dataset(p_y_test, WINDOW_SIZE, BATCH_SIZE, False)\n",
        "\n",
        "    pred = model.predict(pred_test)\n",
        "    pred_df.iloc[-1]['y'] = pred[-1]\n",
        "\n",
        "    data = scaler.inverse_transform(pred_df)\n",
        "    data = pd.DataFrame(data, columns=scale_cols)\n",
        "    data['DATE'] = df_row['DATE']\n",
        "    data = data[['DATE', '거래량', 'PER', 'PBR', '기관 합계', '기타법인', '개인', '외국인 합계', 'ATR',\n",
        "       'NASDAQ', 'S&P', 'CBOE', 'Exchange rate', 'futures2y', 'futures10y',\n",
        "       'y']]\n",
        "  return(data)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbri8J2QIXyD"
      },
      "source": [
        "pred_machine(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GllzW2MyZNEV"
      },
      "source": [
        "schedule.every().hour.do(pred_machine, data)\n",
        "data = pred_machine(data)\n",
        "print(data)\n",
        "\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    data = data\n",
        "    print(data)\n",
        "    time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88Wlf9RYKZ7L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}